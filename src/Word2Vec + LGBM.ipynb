{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer age group prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile word2vec_1.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "train = pd.read_csv('../dataset/L.POINT_train.csv', encoding='UTF-8')\n",
    "test = pd.read_csv('../dataset/L.POINT_test.csv', encoding='UTF-8')\n",
    "\n",
    "p_level = 'CLAC3_NM'\n",
    "\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=False))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "test_corpus = list(test.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "\n",
    "\n",
    "num_features = 3\n",
    "min_word_count = 1 \n",
    "context = 3 \n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(train['CLNT_ID'].unique())}), train_features], axis=1).to_csv('X_train_w2v_CLAC3_NM.csv', index=False)\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(test['CLNT_ID'].unique())}), test_features], axis=1).to_csv('X_test_w2v_CLAC3_NM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(['python', 'word2vec_1.py'], env={**os.environ, 'PYTHONHASHSEED': '123'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLAC2_NM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile word2vec_2.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "train = pd.read_csv('../dataset/L.POINT_train.csv', encoding='UTF-8')\n",
    "test = pd.read_csv('../dataset/L.POINT_test.csv', encoding='UTF-8')\n",
    "\n",
    "p_level = 'CLAC2_NM'\n",
    "\n",
    "\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=False))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "test_corpus = list(test.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "\n",
    "num_features = 30 \n",
    "min_word_count = 1 \n",
    "context = 3 \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(train['CLNT_ID'].unique())}), train_features], axis=1).to_csv('X_train_w2v_CLAC2_NM.csv', index=False)\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(test['CLNT_ID'].unique())}), test_features], axis=1).to_csv('X_test_w2v_CLAC2_NM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['python', 'word2vec_2.py'], env={**os.environ, 'PYTHONHASHSEED': '123'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PD_BRA_NM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile word2vec_3.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "train = pd.read_csv('../dataset/L.POINT_train.csv', encoding='UTF-8')\n",
    "test = pd.read_csv('../dataset/L.POINT_test.csv', encoding='UTF-8')\n",
    "\n",
    "\n",
    "p_level = 'PD_BRA_NM'\n",
    "\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=False))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "test_corpus = list(test.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "\n",
    "num_features = 30 \n",
    "min_word_count = 1 \n",
    "context = 3\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(train['CLNT_ID'].unique())}), train_features], axis=1).to_csv('X_train_w2v_PD_BRA_NM.csv', index=False)\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(test['CLNT_ID'].unique())}), test_features], axis=1).to_csv(X_test_w2v_PD_BRA_NM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['python', 'word2vec_3.py'], env={**os.environ, 'PYTHONHASHSEED': '123'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KWD_NM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile word2vec_4.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "train = pd.read_csv('../dataset/L.POINT_train.csv', encoding='UTF-8')\n",
    "test = pd.read_csv('../dataset/L.POINT_test.csv', encoding='UTF-8')\n",
    "\n",
    "p_level = 'KWD_NM'\n",
    "\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=False))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "test_corpus = list(test.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "\n",
    "num_features = 30 \n",
    "min_word_count = 1 \n",
    "context = 3 \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(train['CLNT_ID'].unique())}), train_features], axis=1).to_csv('X_train_w2v_KWD_NM.csv', index=False)\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(test['CLNT_ID'].unique())}), test_features], axis=1).to_csv('X_test_w2v_KWD_NM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['python', 'word2vec_4.py'], env={**os.environ, 'PYTHONHASHSEED': '123'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PD_ADD_NM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile word2vec_5.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "train = pd.read_csv('../dataset/L.POINT_train.csv', encoding='UTF-8')\n",
    "test = pd.read_csv('../dataset/L.POINT_test.csv', encoding='UTF-8')\n",
    "\n",
    "p_level = 'PD_ADD_NM'\n",
    "\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=False))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "test_corpus = list(test.groupby('CLNT_ID')[p_level].agg(oversample, 30))\n",
    "\n",
    "num_features = 30 \n",
    "min_word_count = 1 \n",
    "context = 3 \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(train['CLNT_ID'].unique())}), train_features], axis=1).to_csv('X_train_w2v_PD_ADD_NM.csv', index=False)\n",
    "pd.concat([pd.DataFrame({'CLNT_ID': np.sort(test['CLNT_ID'].unique())}), test_features], axis=1).to_csv('X_test_w2v_PD_ADD_NM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['python', 'word2vec_5.py'], env={**os.environ, 'PYTHONHASHSEED': '123'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# EDA\n",
    "import klib\n",
    "\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Modeling\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load & Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/L.POINT_train.csv', encoding='UTF-8')\n",
    "test = pd.read_csv('../dataset/L.POINT_test.csv', encoding='UTF-8')\n",
    "\n",
    "y_target =  pd.read_csv('../dataset/y_train.csv').LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v_1 = pd.read_csv(word2vec_path +'X_train_w2v_CLAC3_NM.csv', encoding='cp949')\n",
    "test_w2v_1= pd.read_csv(word2vec_path +'X_test_w2v_CLAC3_NM.csv', encoding='cp949')\n",
    "\n",
    "train_w2v_2  = pd.read_csv(word2vec_path +'X_train_w2v_CLAC2_NM.csv', encoding='cp949')\n",
    "test_w2v_2 = pd.read_csv(word2vec_path +'X_test_w2v_CLAC2_NM.csv', encoding='cp949')\n",
    "\n",
    "train_w2v_3  = pd.read_csv(word2vec_path +'X_train_w2v_PD_BRA_NM.csv', encoding='cp949')\n",
    "test_w2v_3 = pd.read_csv(word2vec_path +'X_test_w2v_PD_BRA_NM.csv', encoding='cp949')\n",
    "\n",
    "train_w2v_4  = pd.read_csv(word2vec_path +'X_train_w2v_KWD_NM.csv', encoding='cp949')\n",
    "test_w2v_4 = pd.read_csv(word2vec_path +'X_test_w2v_KWD_NM.csv', encoding='cp949')\n",
    "\n",
    "train_w2v_5  = pd.read_csv(word2vec_path +'X_train_w2v_PD_ADD_NM.csv', encoding='cp949')\n",
    "test_w2v_5 = pd.read_csv(word2vec_path +'X_test_w2v_PD_ADD_NM.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = pd.concat([train_w2v_1, train_w2v_2, train_w2v_3, train_w2v_4, train_w2v_5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w2v = pd.concat([test_w2v_1, test_w2v_2, test_w2v_3, test_w2v_4, test_w2v_5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v.shape, test_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_tr = train_w2v_1.CLNT_ID\n",
    "cust_te = test_w2v_1.CLNT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_w2v = pd.concat([train_w2v, test_w2v] ,axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_w2v.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_w2v = pd.concat([pd.concat([cust_tr, cust_te]).reset_index(drop=True), features_w2v], axis=1)\n",
    "X_train = features_w2v.query('CLNT_ID in @cust_tr').drop('CLNT_ID', axis=1)\n",
    "X_test = features_w2v.query('CLNT_ID in @cust_te').drop('CLNT_ID', axis=1)\n",
    "\n",
    "model = LogisticRegression(random_state=0)\n",
    "sfk = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "cv_scores = [] \n",
    "for p in tqdm(range(1,100,1)):\n",
    "    X_new = SelectPercentile(percentile=p).fit_transform(X_train, y_target)    \n",
    "    cv_score = cross_val_score(model, X_new, y_target, scoring='neg_log_loss', cv=sfk).mean()\n",
    "    cv_scores.append((p,cv_score))\n",
    "\n",
    "best_score = cv_scores[np.argmax([score for _, score in cv_scores])]\n",
    "print(best_score)\n",
    "\n",
    "plt.plot([k for k, _ in cv_scores], [score for _, score in cv_scores])\n",
    "plt.xlabel('Percent of features')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = SelectPercentile(percentile=best_score[0]).fit(X_train, y_target)\n",
    "X_train = fs.transform(X_train)\n",
    "X_test = fs.transform(X_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터파일 저장\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.to_csv('train_w2v.csv')\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test.to_csv('test_w2v.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_target, test_size=0.3, stratify = y_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_dev.shape, y_train.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = LGBMClassifier(random_state=42)\n",
    "param_grid = {'n_estimators': [100,200,300,400,500],\n",
    "              'objective' : ['multiclass'],\n",
    "             'learning_rate': [0.01,0.1],\n",
    "              'max_depth': [3,4,5,6,10],\n",
    "              'num_leaves': [16,32,64,128]}\n",
    "\n",
    "rcv_lgb = RandomizedSearchCV(model_lgb, param_distributions=param_grid ,cv=skf, scoring='neg_log_loss', n_iter=10)\n",
    "rcv_lgb.fit(X_train,y_train)\n",
    "\n",
    "print('랜덤서치(skf) 점수', -(rcv_lgb.score(X_dev,y_dev)))\n",
    "print('최적 파라미터', rcv_lgb.best_params_)   \n",
    "print('최고 점수', -rcv_lgb.best_score_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Submisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(rcv_lgb.predict_proba(X_test))\n",
    "\n",
    "result = pd.concat([cust_te , pred], axis =1)\n",
    "result.columns = ['CLNT_ID','F20','F30','F40','M20','M30','M40']\n",
    "result\n",
    "\n",
    "result.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
